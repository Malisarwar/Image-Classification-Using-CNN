# -*- coding: utf-8 -*-
"""CV_Assignment_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXxy-MSQZVh5KE_cCq6z34FdVmVt_-ec

**Packges**
"""

import os, shutil
import matplotlib.pyplot as plt
from tensorflow.keras import layers, optimizers
from tensorflow.keras.models import Sequential, load_model
import cv2
import tensorflow as tf
from tensorflow import keras
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix

"""**Directory Path**


"""

root_dir = os.getcwd()
buildings_dir=root_dir+"/data/seg_train/seg_train/buildings"
forest_dir=root_dir+"/data/seg_train/seg_train/forest"
glacier_dir = root_dir+"/data/seg_train/seg_train/glacier"
mountain_dir = root_dir+"/data/seg_train/seg_train/mountain"
sea_dir = root_dir+"/data/seg_train/seg_train/sea"
street_dir = root_dir+"/data/seg_train/seg_train/street"

"""**Getting image file**"""

build_imgs = os.listdir(buildings_dir)
forest_imgs = os.listdir(forest_dir)
gla_imgs = os.listdir(glacier_dir)
mount_imgs = os.listdir(mountain_dir)
sea_imgs = os.listdir(sea_dir)
street_imgs = os.listdir(street_dir)

def readData(X,labels,base_dir,filesname,label):
  for imgFile in filesname:
    imagePath = os.path.join(base_dir,imgFile)
    image = cv2.imread(imagePath)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (210, 210))
    X.append(image)
    labels.append(label)

"""**Reading Training Data**"""

X_train=[]
y_train=[]
readData(X_train,y_train,buildings_dir,build_imgs,"Buildings")
readData(X_train,y_train,forest_dir,forest_imgs,"Forest")
readData(X_train,y_train,glacier_dir,gla_imgs,"Glacier")
readData(X_train,y_train,mountain_dir,mount_imgs,"Mountain")
readData(X_train,y_train,sea_dir,sea_imgs,"Sea")
readData(X_train,y_train,street_dir,street_imgs,"Street")

"""**Reading Test Data**"""

buildings_test_dir=root_dir+"/data/seg_test/seg_test/buildings"
forest_test_dir=root_dir+"/data/seg_test/seg_test/forest"
glacier_test_dir = root_dir+"/data/seg_test/seg_test/glacier"
mountain_test_dir = root_dir+"/data/seg_test/seg_test/mountain"
sea_test_dir = root_dir+"/data/seg_test/seg_test/sea"
street_test_dir = root_dir+"/data/seg_test/seg_test/street"

build_imgs = os.listdir(buildings_test_dir)
forest_imgs = os.listdir(forest_test_dir)
gla_imgs = os.listdir(glacier_test_dir)
mount_imgs = os.listdir(mountain_test_dir)
sea_imgs = os.listdir(sea_test_dir)
street_imgs = os.listdir(street_test_dir)

X_test=[]
y_test=[]
readData(X_test,y_test,buildings_test_dir,build_imgs,"Buildings")
readData(X_test,y_test,forest_test_dir,forest_imgs,"Forest")
readData(X_test,y_test,glacier_test_dir,gla_imgs,"Glacier")
readData(X_test,y_test,mountain_test_dir,mount_imgs,"Mountain")
readData(X_test,y_test,sea_test_dir,sea_imgs,"Sea")
readData(X_test,y_test,street_test_dir,street_imgs,"Street")

lb = preprocessing.LabelBinarizer()
labels = lb.fit_transform(y_train)
X_train=np.array(X_train,dtype="float32")
y_train=np.array(labels)

# flabels = np.flatten(labels)
print("X_train shape",X_train.shape)
# print("f la shape",flabels)
print("y_train labels shape:",y_train.shape)

lb = preprocessing.LabelBinarizer()
labels = lb.fit_transform(y_test)
X_test=np.array(X_test,dtype="float32")
y_test=np.array(labels)

# flabels = np.flatten(labels)
print("X_test shape",X_test.shape)
# print("f la shape",flabels)
print("y_test labels shape:",y_test.shape)

model = Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',
                        input_shape=(210, 210, 3)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))

model.add(layers.Conv2D(256, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(6,activation='softmax'))

model.summary()

"""##Stochastic Gradient Decent with learning rate##"""

sgd = optimizers.SGD(lr=0.0001, decay=1e-3, momentum=0.5)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['acc'])

print("Y_Train shape:",y_train.shape)
print("X_Train shape:",X_train.shape)

"""##Model training with validation split of 10%##"""

history = model.fit(X_train,y_train,epochs=80,batch_size=64, verbose=1,validation_split=0.10)

model.evaluate(X_test,y_test,batch_size=128)
 y_pred = model.predict(X_test)

print("Y_Test shape:",y_test.shape)
print("Y_pred shape:",y_pred.shape)
y_test1=np.argmax(y_test, axis=1)
y_pred1 = np.argmax(y_pred, axis=1)
confusion_matrix = confusion_matrix(y_test1, y_pred1)

labels.shape

print(confusion_matrix)
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(confusion_matrix)
plt.title('Confusion matrix of the classifier')
fig.colorbar(cax)
ax.set_xticklabels([''])
ax.set_yticklabels([''])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""##Save model##"""

model.save("test_accurracy 85.h5")

"""##Graphs of loss and accuracy##"""

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()